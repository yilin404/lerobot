from collections import deque

import numpy as np
import einops
import torch
from torch import nn
from torch.nn import functional as F
import torchvision
import torchvision.models._utils
import math

from huggingface_hub import PyTorchModelHubMixin
from diffusers.schedulers.scheduling_ddim import DDIMScheduler
from diffusers.schedulers.scheduling_ddpm import DDPMScheduler

from lerobot.common.policies.rdt.configuration_rdt import RDTConfig
from lerobot.common.policies.normalize import Normalize, Unnormalize
from lerobot.common.policies.utils import (
    get_device_from_parameters,
    get_dtype_from_parameters,
    populate_queues,
)

# typing
from torch import Tensor
from typing import Callable, Optional, Union, Dict, Tuple

class RDTPolicy(
    nn.Module,
    PyTorchModelHubMixin,
    library_name="lerobot",
    repo_url="https://github.com/huggingface/lerobot",
    tags=["robotics", "diffusion-policy"],
):
    name = "rdt"

    def __init__(
        self,
        config: Optional[RDTConfig] = None,
        dataset_stats: Optional[Dict[str, Dict[str, Tensor]]] = None,
    ):
        """
        Args:
            config: Policy configuration class instance or None, in which case the default instantiation of
                the configuration class is used.
            dataset_stats: Dataset statistics to be used for normalization. If not passed here, it is expected
                that they will be passed with a call to `load_state_dict` before the policy is used.
        """
        super().__init__()
        if config is None:
            config = RDTConfig()
        self.config = config
        self.normalize_inputs = Normalize(
            config.input_shapes, config.input_normalization_modes, dataset_stats
        )
        self.normalize_targets = Normalize(
            config.output_shapes, config.output_normalization_modes, dataset_stats
        )
        self.unnormalize_outputs = Unnormalize(
            config.output_shapes, config.output_normalization_modes, dataset_stats
        )

        # queues are populated during rollout of the policy, they contain the n latest observations and actions
        self._queues = None

        self.diffusion = DiffusionModel(config.n_obs_steps, config.horizon, config.n_action_steps,
                                        config.input_shapes, config.output_shapes,
                                        config.crop_shape, config.crop_is_random,
                                        config.vision_backbone, config.pretrained_backbone_weights,
                                        config.use_group_norm,
                                        config.condition_embed_dim,
                                        config.down_dims, config.kernel_size, config.n_groups,
                                        config.num_heads, config.dropout,
                                        config.noise_scheduler_type, config.num_train_timesteps,
                                        config.beta_schedule, config.beta_start, config.beta_end,
                                        config.prediction_type, config.clip_sample, config.clip_sample_range,
                                        config.num_inference_steps, config.do_mask_loss_for_padding)

        self.expected_image_keys = [k for k in config.input_shapes if k.startswith("observation.image")]
        self.use_env_state = "observation.environment_state" in config.input_shapes

        self.reset()

    def reset(self):
        """Clear observation and action queues. Should be called on `env.reset()`"""
        self._queues = {
            "observation.state": deque(maxlen=self.config.n_obs_steps),
            "action": deque(maxlen=self.config.n_action_steps),
        }
        if len(self.expected_image_keys) > 0:
            self._queues["observation.images"] = deque(maxlen=self.config.n_obs_steps)
        if self.use_env_state:
            self._queues["observation.environment_state"] = deque(maxlen=self.config.n_obs_steps)
    
    @torch.no_grad
    def select_action(self, batch: dict[str, Tensor]) -> Tensor:
        """Select a single action given environment observations.

        This method handles caching a history of observations and an action trajectory generated by the
        underlying diffusion model. Here's how it works:
          - `n_obs_steps` steps worth of observations are cached (for the first steps, the observation is
            copied `n_obs_steps` times to fill the cache).
          - The diffusion model generates `horizon` steps worth of actions.
          - `n_action_steps` worth of actions are actually kept for execution, starting from the current step.
        Schematically this looks like:
            ----------------------------------------------------------------------------------------------
            (legend: o = n_obs_steps, h = horizon, a = n_action_steps)
            |timestep            | n-o+1 | n-o+2 | ..... | n     | ..... | n+a-1 | n+a   | ..... | n-o+h |
            |observation is used | YES   | YES   | YES   | YES   | NO    | NO    | NO    | NO    | NO    |
            |action is generated | YES   | YES   | YES   | YES   | YES   | YES   | YES   | YES   | YES   |
            |action is used      | NO    | NO    | NO    | YES   | YES   | YES   | NO    | NO    | NO    |
            ----------------------------------------------------------------------------------------------
        Note that this means we require: `n_action_steps <= horizon - n_obs_steps + 1`. Also, note that
        "horizon" may not the best name to describe what the variable actually means, because this period is
        actually measured from the first observation which (if `n_obs_steps` > 1) happened in the past.
        """
        batch = self.normalize_inputs(batch)
        if len(self.expected_image_keys) > 0:
            batch = dict(batch)  # shallow copy so that adding a key doesn't modify the original
            batch["observation.images"] = torch.stack([batch[k] for k in self.expected_image_keys], dim=-4)
        # Note: It's important that this happens after stacking the images into a single key.
        self._queues = populate_queues(self._queues, batch)

        if len(self._queues["action"]) == 0:
            # stack n latest observations from the queue
            batch = {k: torch.stack(list(self._queues[k]), dim=1) for k in batch if k in self._queues}
            actions = self.diffusion.generate_actions(batch)

            # TODO(rcadene): make above methods return output dictionary?
            actions = self.unnormalize_outputs({"action": actions})["action"]

            self._queues["action"].extend(actions.transpose(0, 1))

        action = self._queues["action"].popleft()
        return action

    def forward(self, batch: dict[str, Tensor]) -> dict[str, Tensor]:
        """Run the batch through the model and compute the loss for training or validation."""
        batch = self.normalize_inputs(batch)
        if len(self.expected_image_keys) > 0:
            batch = dict(batch)  # shallow copy so that adding a key doesn't modify the original
            batch["observation.images"] = torch.stack([batch[k] for k in self.expected_image_keys], dim=-4)
        batch = self.normalize_targets(batch)
        loss = self.diffusion.compute_loss(batch)
        return {"loss": loss}

def _make_noise_scheduler(name: str, **kwargs: dict) -> Union[DDPMScheduler, DDIMScheduler]:
    """
    Factory for noise scheduler instances of the requested type. All kwargs are passed
    to the scheduler.
    """
    if name == "DDPM":
        return DDPMScheduler(**kwargs)
    elif name == "DDIM":
        return DDIMScheduler(**kwargs)
    else:
        raise ValueError(f"Unsupported noise scheduler type {name}")

class DiffusionModel(nn.Module):
    def __init__(self,
                 # inputs / outputs structure params
                 n_obs_steps: int,
                 horizon: int,
                 n_action_steps: int,
                 input_shapes: dict[str, list[int]],
                 output_shapes: dict[str, list[int]],
                 # vision encoder params
                 crop_shape: Optional[Tuple[int, int]],
                 crop_is_random: bool,
                 vision_backbone: str,
                 pretrained_backbone_weights: Optional[str],
                 use_group_norm: bool,
                 # conditional unet params
                 condition_embed_dim: int,
                 down_dims : tuple[int, ...],
                 kernel_size: int,
                 n_groups: int,
                 num_heads: int,
                 dropout: float,
                 # noise scheduler params 
                 noise_scheduler_type: str,
                 num_train_timesteps: int,
                 beta_schedule: str,
                 beta_start: float,
                 beta_end: float,
                 prediction_type: str,
                 clip_sample: bool,
                 clip_sample_range: float,
                 # inference params
                 num_inference_steps: Optional[int],
                 # Loss computation
                 do_mask_loss_for_padding: bool):
        super().__init__()

        self.n_obs_steps = n_obs_steps
        self.horizon = horizon
        self.n_action_steps = n_action_steps
        self.input_shapes = input_shapes
        self.output_shapes = output_shapes

        self.condition_embed_dim = condition_embed_dim

        # condition encoder
        self.state_encoder = MLPEncoder(self.input_shapes["observation.state"][0],
                                        self.condition_embed_dim,
                                        self.condition_embed_dim * 4)
        self._use_images = False
        self._use_env_state = False
        if len([k for k in self.input_shapes if k.startswith("observation.image")]) > 0:
            self._use_images = True
            self.vision_encoder = VisionEncoder(crop_shape, crop_is_random, 
                                                vision_backbone, pretrained_backbone_weights, 
                                                use_group_norm, 
                                                self.condition_embed_dim)
        if "observation.environment_state" in self.input_shapes:
            self._use_env_state = True
            self.env_state_encoder = MLPEncoder(self.input_shapes["observation.environment_state"][0],
                                                self.condition_embed_dim,
                                                self.condition_embed_dim * 4)
        self.diffusion_step_encoder = DiffusionStepEncoder(self.condition_embed_dim)

        # conditional unet
        self.unet = ConditionalUnet1d(output_shapes=self.output_shapes,
                                      condition_embed_dim=self.condition_embed_dim,
                                      down_dims=down_dims,
                                      kernel_size=kernel_size, n_groups=n_groups,
                                      num_heads=num_heads,
                                      dropout=dropout,)

        self.noise_scheduler = _make_noise_scheduler(
            noise_scheduler_type,
            num_train_timesteps=num_train_timesteps,
            beta_schedule=beta_schedule,
            beta_start=beta_start,
            beta_end=beta_end,
            prediction_type=prediction_type,
            clip_sample=clip_sample,
            clip_sample_range=clip_sample_range,
        )

        if num_inference_steps is None:
            self.num_inference_steps = self.noise_scheduler.config.num_train_timesteps
        else:
            self.num_inference_steps = num_inference_steps

        self.do_mask_loss_for_padding = do_mask_loss_for_padding
    
    # ========= inference  ============
    def conditional_sample(
        self, batch_size: int, global_cond: Tensor, generator: Optional[torch.Generator] = None
    ) -> Tensor:
        device = get_device_from_parameters(self)
        dtype = get_dtype_from_parameters(self)

        # Sample prior.
        sample = torch.randn(
            size=(batch_size, self.horizon, self.output_shapes["action"][0]),
            dtype=dtype,
            device=device,
            generator=generator,
        )

        self.noise_scheduler.set_timesteps(self.num_inference_steps)

        for t in self.noise_scheduler.timesteps:
            diffusion_step = torch.full(sample.shape[:1], t, dtype=torch.long, device=sample.device)
            diffusion_step_embed = self.diffusion_step_encoder(diffusion_step)
            # Predict model output.
            model_output = self.unet(
                sample,
                diffusion_step_embed=diffusion_step_embed,
                global_cond=global_cond,
            )
            # Compute previous image: x_t -> x_t-1
            sample = self.noise_scheduler.step(model_output, t, sample, generator=generator).prev_sample

        return sample

    def _prepare_global_conditioning(self, batch: dict[str, Tensor]) -> Tensor:
        """Encode image features and concatenate them all together along with the state vector."""
        batch_size, n_obs_steps = batch["observation.state"].shape[:2]

        global_cond_feats = [self.state_encoder(batch["observation.state"])]
        global_cond_shapes = [[n_obs_steps]]
        # Extract image features.
        if self._use_images:
            # Combine batch, sequence, and "which camera" dims before passing to shared encoder.
            img_features = self.vision_encoder(
                einops.rearrange(batch["observation.images"], "b s n ... -> (b s n) ...")
            )
            # Separate batch dim and sequence dim back out. The camera index dim gets absorbed into the
            # feature dim (effectively concatenating the camera features).
            img_features = einops.rearrange(
                img_features, "(b s n) h w c -> b s n h w c", b=batch_size, s=n_obs_steps
            )
            global_cond_feats.append(einops.rearrange(img_features, "b s n h w c -> b (s n h w) c"))
            global_cond_shapes.append(list(img_features.shape[1:-1]))

        if self._use_env_state:
            global_cond_feats.append(self.env_state_encoder(batch["observation.environment_state"]))
            global_cond_shapes.append([n_obs_steps])

        device = get_device_from_parameters(self)
        dtype = get_dtype_from_parameters(self)
        global_cond_pos_embeds = get_multimodel_cond_pos_embed(embed_dim=self.condition_embed_dim, mm_cond_shapes=global_cond_shapes)
        global_cond_pos_embeds = [torch.from_numpy(pos_embed).to(device=device, dtype=dtype).unsqueeze(0).expand(batch_size, -1, -1) for pos_embed in global_cond_pos_embeds]

        # Concatenate features and position embedings
        return torch.cat(global_cond_feats, dim=-2) + torch.cat(global_cond_pos_embeds, dim=-2)

    def generate_actions(self, batch: dict[str, Tensor]) -> Tensor:
        """
        This function expects `batch` to have:
        {
            "observation.state": (B, n_obs_steps, state_dim)

            "observation.images": (B, n_obs_steps, num_cameras, C, H, W)
                AND/OR
            "observation.environment_state": (B, environment_dim)
        }
        """
        batch_size, n_obs_steps = batch["observation.state"].shape[:2]
        assert n_obs_steps == self.n_obs_steps

        # Encode image features and concatenate them all together along with the state vector.
        global_cond = self._prepare_global_conditioning(batch)  # (B, L, condition_embed_dim)

        # run sampling
        actions = self.conditional_sample(batch_size, global_cond=global_cond)

        # Extract `n_action_steps` steps worth of actions (from the current observation).
        start = n_obs_steps - 1
        end = start + self.n_action_steps
        actions = actions[:, start:end]

        return actions

    def compute_loss(self, batch: dict[str, Tensor]) -> Tensor:
        """
        This function expects `batch` to have (at least):
        {
            "observation.state": (B, n_obs_steps, state_dim)

            "observation.images": (B, n_obs_steps, num_cameras, C, H, W)
                AND/OR
            "observation.environment_state": (B, environment_dim)

            "action": (B, horizon, action_dim)
            "action_is_pad": (B, horizon)
        }
        """
        # Input validation.
        assert set(batch).issuperset({"observation.state", "action", "action_is_pad"})
        assert "observation.images" in batch or "observation.environment_state" in batch
        n_obs_steps = batch["observation.state"].shape[1]
        horizon = batch["action"].shape[1]
        assert horizon == self.horizon
        assert n_obs_steps == self.n_obs_steps

        # Encode image features and concatenate them all together along with the state vector.
        global_cond = self._prepare_global_conditioning(batch)  # (B, L, condition_embed_dim)

        # Forward diffusion.
        trajectory = batch["action"]
        # Sample noise to add to the trajectory.
        eps = torch.randn(trajectory.shape, device=trajectory.device)
        # Sample a random noising timestep for each item in the batch.
        timesteps = torch.randint(
            low=0,
            high=self.noise_scheduler.config.num_train_timesteps,
            size=(trajectory.shape[0],),
            device=trajectory.device,
        ).long()
        # Add noise to the clean trajectories according to the noise magnitude at each timestep.
        noisy_trajectory = self.noise_scheduler.add_noise(trajectory, eps, timesteps)

        # Run the denoising network (that might denoise the trajectory, or attempt to predict the noise).
        timesteps_embed = self.diffusion_step_encoder(timesteps)
        pred = self.unet(noisy_trajectory,
                         timesteps_embed, 
                         global_cond=global_cond,)

        # Compute the loss.
        # The target is either the original trajectory, or the noise.
        if self.noise_scheduler.config.prediction_type == "epsilon":
            target = eps
        elif self.noise_scheduler.config.prediction_type == "sample":
            target = batch["action"]
        else:
            raise ValueError(f"Unsupported prediction type {self.noise_scheduler.config.prediction_type}")

        loss = F.mse_loss(pred, target, reduction="none")

        # Mask loss wherever the action is padded with copies (edges of the dataset trajectory).
        if self.do_mask_loss_for_padding:
            if "action_is_pad" not in batch:
                raise ValueError(
                    "You need to provide 'action_is_pad' in the batch when "
                    f"{self.do_mask_loss_for_padding=}."
                )
            in_episode_bound = ~batch["action_is_pad"]
            loss = loss * in_episode_bound.unsqueeze(-1)

        return loss.mean()
    
def get_1d_sincos_pos_embed(embed_dim: int, pos: np.ndarray, max_period: int=10000) -> np.ndarray:
    """
    embed_dim: output dimension for each position
    pos: a list of positions to be encoded: size (M,)
    out: (M, D)
    """
    assert embed_dim % 2 == 0, "Embedding dimension must be even."
    omega = np.arange(embed_dim // 2, dtype=np.float64)
    omega /= embed_dim / 2.
    omega = 1. / (max_period ** omega) # (D/2,)

    pos = pos.reshape(-1) # (M,)
    out = np.einsum("m,d->md", pos, omega) # (M, D/2), outer product

    emb_sin = np.sin(out) # (M, D/2)
    emb_cos = np.cos(out) # (M, D/2)

    emb = np.concatenate([emb_sin, emb_cos], axis=1)  # (M, D)
    return emb

def get_nd_sincos_pos_embed_from_grid(embed_dim: int, grid_size: list[int]) -> np.ndarray:
    """
    embed_dim: output dimension for each position
    grid_size: the grids sizes in each dimension (K,).
    out: (grid_sizes[0], ..., grid_sizes[K-1], D)
    """
    num_size = len(grid_size)
    emb = np.zeros(grid_size + [embed_dim,])
    # Uniformly divide the embedding dimension for each grid size
    dim_for_each_grid = embed_dim // num_size
     # To make it even
    if dim_for_each_grid % 2 != 0:
        dim_for_each_grid -= 1
    for size_idx in range(num_size):
        size = grid_size[size_idx]
        if size <= 1:
            continue
        pos = np.arange(size)
        posemb_shape = [1] * num_size + [dim_for_each_grid]
        posemb_shape[size_idx] = -1
        emb[..., size_idx * dim_for_each_grid:(size_idx + 1) * dim_for_each_grid] += \
            get_1d_sincos_pos_embed(dim_for_each_grid, pos).reshape(posemb_shape)
    return emb

def get_multimodel_cond_pos_embed(embed_dim:int, 
                                  mm_cond_shapes: list[list[int]], 
                                  embed_modality: bool = True) -> np.ndarray:
    """
    Generate position embeddings for multimodal conditions. 
    
    embed_dim: output dimension for each position
    mm_cond_lens: an Dict containing 
        (modality name, modality token length) pairs.
        For `"image"` modality, the value can be a multi-dimensional tuple.
    embed_modality: whether to embed the modality information. Default is True.
    """
    num_modality = len(mm_cond_shapes)
    modality_pos_embed = np.zeros((num_modality, embed_dim))
    if embed_modality:
        # Get embeddings for various modalites
        # We put it in the first half
        modality_sincos_embed = get_1d_sincos_pos_embed(
            embed_dim // 2, torch.arange(num_modality))
        modality_pos_embed[:, :embed_dim // 2] = modality_sincos_embed
        # The second half is for position embeddings
        pos_embed_dim = embed_dim // 2
    else:
        # The whole embedding is for position embeddings
        pos_embed_dim = embed_dim
    
    # Get embeddings for positions inside each modality
    mm_cond_pos_embed = list()
    for idx, cond_shape in enumerate(mm_cond_shapes):
        cond_pos_sincos_embed = np.zeros(cond_shape + [embed_dim,])
        cond_pos_sincos_embed[..., -pos_embed_dim:] = get_nd_sincos_pos_embed_from_grid(pos_embed_dim, cond_shape)
        cond_pos_sincos_embed = cond_pos_sincos_embed.reshape(-1, embed_dim)
        cond_pos_sincos_embed += modality_pos_embed[[idx]]
        
        mm_cond_pos_embed.append(cond_pos_sincos_embed)
    
    return mm_cond_pos_embed

class DiffusionStepEncoder(nn.Module):
    def __init__(self, embed_channels: int, max_period: int = 10000):
        super().__init__()

        self.embed_channels = embed_channels
        self.max_period = max_period

        self.encoder = nn.Sequential(
            nn.Linear(embed_channels, embed_channels * 4),
            nn.SiLU(),
            nn.Linear(embed_channels * 4, embed_channels),
        )
    
    def _timestep_embedding(self, t: Tensor, dim: int, max_period: int):
        """
        Create sinusoidal timestep embeddings.
        :param t: a 1-D Tensor of N indices, one per batch element.
                          These may be fractional.
        :param dim: the dimension of the output.
        :param max_period: controls the minimum frequency of the embeddings.
        :return: an (N, D) Tensor of positional embeddings.
        """
        # https://github.com/openai/glide-text2im/blob/main/glide_text2im/nn.py
        half = dim // 2
        freqs = torch.exp(
            -math.log(max_period) * torch.arange(
                start=0, end=half, dtype=torch.float32, device=t.device) / half
        )
        args = t[:, None].float() * freqs[None]
        embedding = torch.cat([torch.sin(args), torch.cos(args)], dim=-1)
        if dim % 2:
            embedding = torch.cat([embedding, torch.zeros_like(embedding[:, :1])], dim=-1)
        return embedding.to(get_dtype_from_parameters(self))
    
    def forward(self, t: Tensor):
        return self.encoder(self._timestep_embedding(t, self.embed_channels, self.max_period))

class MLPEncoder(nn.Module):
    def __init__(self, inp_channels: int, out_channels: int, hidden_channels: int):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(inp_channels, hidden_channels),
            nn.SiLU(),
            nn.Linear(hidden_channels, out_channels),
        )

    def forward(self, x: Tensor) -> Tensor:
        return self.net(x)
    
class VisionEncoder(nn.Module):
    def __init__(self,
                 # image crop params 
                 crop_shape: tuple[int, int] | None,
                 crop_is_random: bool,
                 # vision backbone params
                 vision_backbone: str,
                 pretrained_backbone_weights: str | None,
                 use_group_norm: bool,
                 # vision embed params
                 vision_embed_dim: int,):
        super().__init__()
        # Set up optional preprocessing.
        if crop_shape is not None:
            self.do_crop = True
            # Always use center crop for eval
            self.center_crop = torchvision.transforms.CenterCrop(crop_shape)
            if crop_is_random:
                self.maybe_random_crop = torchvision.transforms.RandomCrop(crop_shape)
            else:
                self.maybe_random_crop = self.center_crop
        else:
            self.do_crop = False

        # Set up backbone.
        backbone_model = getattr(torchvision.models, vision_backbone)(
            weights=pretrained_backbone_weights,
            norm_layer=torchvision.ops.misc.FrozenBatchNorm2d if pretrained_backbone_weights is not None else None,
        )
        # Note: The assumption here is that we are using a ResNet model (and hence layer4 is the final
        # feature map).
        # Note: The forward method of this returns a dict: {"feature_map": output}.
        self.backbone = torchvision.models._utils.IntermediateLayerGetter(backbone_model, return_layers={"layer4": "feature_map"})
        if use_group_norm:
            if pretrained_backbone_weights is not None:
                raise ValueError(
                    "You can't replace BatchNorm in a pretrained model without ruining the weights!"
                )
            self.backbone = _replace_submodules(
                root_module=self.backbone,
                predicate=lambda x: isinstance(x, nn.BatchNorm2d),
                func=lambda x: nn.GroupNorm(num_groups=x.num_features // 16, num_channels=x.num_features),
            )
        
        self.embed_proj = nn.Conv2d(backbone_model.fc.in_features, vision_embed_dim, kernel_size=1)

    def forward(self, x: Tensor) -> Tensor:
        """
        Args:
            x: (B, C, H, W) image tensor with pixel values in [0, 1].
        Returns:
            (B, H, W, C) image feature.
        """
        # Preprocess: maybe crop (if it was set up in the __init__).
        if self.do_crop:
            if self.training:  # noqa: SIM108
                x = self.maybe_random_crop(x)
            else:
                # Always use center crop for eval.
                x = self.center_crop(x)
        # Extract backbone feature.
        x = self.backbone(x)["feature_map"]
        x = self.embed_proj(x)
        x = einops.rearrange(x, "b c h w -> b h w c")
        return x

def _replace_submodules(
    root_module: nn.Module, predicate: Callable[[nn.Module], bool], func: Callable[[nn.Module], nn.Module]
) -> nn.Module:
    """
    Args:
        root_module: The module for which the submodules need to be replaced
        predicate: Takes a module as an argument and must return True if the that module is to be replaced.
        func: Takes a module as an argument and returns a new module to replace it with.
    Returns:
        The root module with its submodules replaced.
    """
    if predicate(root_module):
        return func(root_module)

    replace_list = [k.split(".") for k, m in root_module.named_modules(remove_duplicate=True) if predicate(m)]
    for *parents, k in replace_list:
        parent_module = root_module
        if len(parents) > 0:
            parent_module = root_module.get_submodule(".".join(parents))
        if isinstance(parent_module, nn.Sequential):
            src_module = parent_module[int(k)]
        else:
            src_module = getattr(parent_module, k)
        tgt_module = func(src_module)
        if isinstance(parent_module, nn.Sequential):
            parent_module[int(k)] = tgt_module
        else:
            setattr(parent_module, k, tgt_module)
    # verify that all BN are replaced
    assert not any(predicate(m) for _, m in root_module.named_modules(remove_duplicate=True))
    return root_module

class ConditionalUnet1d(nn.Module):
    def __init__(self,
                 # input / output structure params
                 output_shapes: dict[str, list[int]],
                 # unet params
                 condition_embed_dim: int,
                 down_dims: tuple[int, ...],
                 kernel_size: int,
                 n_groups: int,
                 num_heads: int,
                 dropout: float):
        super().__init__()
        
        # In channels / out channels for each downsampling block in the Unet's encoder. For the decoder, we
        # just reverse these.
        in_out = [(condition_embed_dim, down_dims[0])] + list(
            zip(down_dims[:-1], down_dims[1:], strict=True)
        )

        # Stem
        self.stem = nn.Conv1d(output_shapes["action"][0], condition_embed_dim, kernel_size=1)

        # Unet encoder
        common_res_block_kwargs = {
            "cond_channels": condition_embed_dim,
            "kernel_size": kernel_size,
            "n_groups": n_groups,
        }
        common_cond_block_kwargs = {
            "cond_channels": condition_embed_dim,
            "kernel_size": kernel_size,
            "n_groups": n_groups,
            "num_heads": num_heads,
            "dropout": dropout,
        }
        self.down_modules = nn.ModuleList([])
        for ind, (dim_in, dim_out) in enumerate(in_out):
            is_last = ind >= (len(in_out) - 1)
            self.down_modules.append(
                nn.ModuleList(
                    [
                        ResidualBlock1d(dim_in, dim_out, **common_res_block_kwargs),
                        ConditionalBlock1d(dim_out, dim_out, **common_cond_block_kwargs),
                        ResidualBlock1d(dim_out, dim_out, **common_res_block_kwargs),
                        ConditionalBlock1d(dim_out, dim_out, **common_cond_block_kwargs),
                        # Downsample as long as it is not the last block.
                        nn.Conv1d(dim_out, dim_out, kernel_size=3, stride=2, padding=1) if not is_last else nn.Identity(),
                    ]
                )
            )
        
        # Processing in the middle of the auto-encoder.
        self.mid_modules = nn.ModuleList(
            [
                ResidualBlock1d(down_dims[-1], down_dims[-1], **common_res_block_kwargs),
                ConditionalBlock1d(down_dims[-1], down_dims[-1], **common_cond_block_kwargs),
                ResidualBlock1d(down_dims[-1], down_dims[-1], **common_res_block_kwargs),
                ConditionalBlock1d(down_dims[-1], down_dims[-1], **common_cond_block_kwargs),
            ]
        )

        # Unet decoder.
        self.up_modules = nn.ModuleList([])
        for ind, (dim_out, dim_in) in enumerate(reversed(in_out[1:])):
            is_last = ind >= (len(in_out) - 1)
            self.up_modules.append(
                nn.ModuleList(
                    [
                        # dim_in * 2, because it takes the encoder's skip connection as well
                        ResidualBlock1d(dim_in * 2, dim_out, **common_res_block_kwargs),
                        ConditionalBlock1d(dim_out, dim_out, **common_cond_block_kwargs),
                        ResidualBlock1d(dim_out, dim_out, **common_res_block_kwargs),
                        ConditionalBlock1d(dim_out, dim_out, **common_cond_block_kwargs),
                        # Upsample as long as it is not the last block.
                        nn.ConvTranspose1d(dim_out, dim_out, kernel_size=4, stride=2, padding=1) if not is_last else nn.Identity(),
                    ]
                )
            )

        self.final_conv = nn.Sequential(
            ConvBlock1d(down_dims[0], condition_embed_dim, kernel_size=kernel_size, n_groups=n_groups),
            nn.Conv1d(condition_embed_dim, output_shapes["action"][0], 1),
        )
    
    def forward(self, 
                x: Tensor, 
                diffusion_step_embed: Tensor,
                global_cond: Tensor,) -> Tensor:
        """
        Args:
            x: (B, T, input_dim) tensor for input to the Unet.
            diffusion_step_embed: (B, condition_embed_dim) embedding for the diffusion step.
            global_cond: (B, L, condition_embed_dim)
        Returns:
            (B, T, input_dim) diffusion model prediction.
        """
        # For 1D convolutions we'll need feature dimension first.
        x = einops.rearrange(x, "b t c -> b c t")

        x = self.stem(x)

        # Run encoder, keeping track of skip features to pass to the decoder.
        encoder_skip_features: list[Tensor] = []
        for resblock, condblock, resblock2, condblock2, downsample in self.down_modules:
            x = resblock(x, diffusion_step_embed)
            x = condblock(x, global_cond)
            x = resblock2(x, diffusion_step_embed)
            x = condblock2(x, global_cond)
            encoder_skip_features.append(x)
            x = downsample(x)

        x = self.mid_modules[0](x, diffusion_step_embed)
        x = self.mid_modules[1](x, global_cond)
        x = self.mid_modules[2](x, diffusion_step_embed)
        x = self.mid_modules[3](x, global_cond)

        # Run decoder, using the skip features from the encoder.
        for resblock, condblock, resblock2, condblock2, upsample in self.up_modules:
            x = torch.cat((x, encoder_skip_features.pop()), dim=1)
            x = resblock(x, diffusion_step_embed)
            x = condblock(x, global_cond)
            x = resblock2(x, diffusion_step_embed)
            x = condblock2(x, global_cond)
            x = upsample(x)

        x = self.final_conv(x)

        x = einops.rearrange(x, "b c t -> b t c")
        return x

class ConvBlock1d(nn.Module):
    """RMSNorm --> SiLU --> Conv1d"""

    def __init__(self, inp_channels: int, out_channels: int, kernel_size: int, n_groups: int):
        super().__init__()

        self.block = nn.Sequential(
            nn.GroupNorm(n_groups, inp_channels),
            nn.SiLU(),
            nn.Conv1d(inp_channels, out_channels, kernel_size, padding=kernel_size // 2),
        )

    def forward(self, x: Tensor):
        return self.block(x)

class ResidualBlock1d(nn.Module):
    def __init__(self, inp_channels: int, out_channels: int, cond_channels: int, kernel_size: int, n_groups: int):
        super().__init__()

        self.conv1 = ConvBlock1d(inp_channels, out_channels, kernel_size, n_groups)
        self.conv2 = ConvBlock1d(out_channels, out_channels, kernel_size, n_groups)
        self.residual_conv = (
            nn.Conv1d(inp_channels, out_channels, 1) if inp_channels != out_channels else nn.Identity()
        )

        self.diffusion_step_block = nn.Sequential(
            nn.SiLU(),
            nn.Linear(cond_channels, out_channels),
        )
    
    def forward(self, x: Tensor, diffusion_step_embed: Tensor) -> Tensor:
        """
        Perform a forward pass through the ResidualBlock1d.

        Args:
            x (Tensor): Input tensor of shape (batch_size, inp_channels, length).
            diffusion_step_embed (Tensor): Embedding tensor of shape (batch_size, cond_channels) for the diffusion step.

        Returns:
            Tensor: Output tensor after applying the residual block operations.
        """
        out = self.conv1(x) + self.diffusion_step_block(diffusion_step_embed).unsqueeze(-1)
        out = self.conv2(out)
        out = out + self.residual_conv(x)
        return out

class MultiHeadCrossAttention(nn.Module):
    def __init__(
            self,
            inp_channels: int,
            cond_channels: int,
            num_heads: int = 8,
            qkv_bias: bool = False,
            qk_norm: bool = False,
            attn_drop: float = 0,
            proj_drop: float = 0,
            norm_layer: nn.Module = nn.RMSNorm,) -> None:
        super().__init__()
        assert inp_channels % num_heads == 0, 'inp_channels should be divisible by num_heads'
        self.num_heads = num_heads
        self.head_channels = inp_channels // num_heads
        self.scale = self.head_channels ** -0.5

        self.q = nn.Linear(inp_channels, inp_channels, bias=qkv_bias)
        self.kv = nn.Linear(cond_channels, inp_channels * 2, bias=qkv_bias)
        self.q_norm = norm_layer(self.head_channels) if qk_norm else nn.Identity()
        self.k_norm = norm_layer(self.head_channels) if qk_norm else nn.Identity()
        self.attn_drop = nn.Dropout(attn_drop)
        self.proj = nn.Linear(inp_channels, inp_channels)
        self.proj_drop = nn.Dropout(proj_drop)
    
    def forward(self, x: torch.Tensor, cond: torch.Tensor, 
                mask: torch.Tensor | None = None) -> torch.Tensor:
        """
        Args:
            x: (B, N, inp_channels) query
            cond: (B, L, cond_channels) key and value
            mask: (B, L) mask for the condition
        """
        B, N, C = x.shape
        _, L, _ = cond.shape
        q = self.q(x).reshape(B, N, self.num_heads, self.head_channels).permute(0, 2, 1, 3)
        kv = self.kv(cond).reshape(B, L, 2, self.num_heads, self.head_channels).permute(2, 0, 3, 1, 4)
        k, v = kv.unbind(0)
        q, k = self.q_norm(q), self.k_norm(k)

        # Prepare attn mask (B, L) to mask the conditioion
        if mask is not None:
            mask = mask.reshape(B, 1, 1, L)
            mask = mask.expand(-1, -1, N, -1)
        
        x = F.scaled_dot_product_attention(
            query=q,
            key=k,
            value=v,
            dropout_p=self.attn_drop.p if self.training else 0.,
            attn_mask=mask
        )
            
        x = x.permute(0, 2, 1, 3).reshape(B, N, C)
        x = self.proj(x)
        if self.proj_drop.p > 0:
            x = self.proj_drop(x)
        return x

class ConditionalBlock1d(nn.Module):
    def __init__(self, 
                 inp_channels: int, out_channels: int, cond_channels: int, 
                 kernel_size: int, n_groups: int,
                 num_heads: int, dropout: float):
        super().__init__()

        self.norm = nn.GroupNorm(n_groups, inp_channels)
        self.conv1 = nn.Conv1d(inp_channels, out_channels, kernel_size, padding=kernel_size // 2)
        # cross attention
        self.norm1 = nn.RMSNorm(out_channels)
        self.cross_attention = MultiHeadCrossAttention(out_channels, cond_channels, num_heads, 
                                                       qkv_bias=True, qk_norm=True,
                                                       norm_layer=nn.RMSNorm)
        self.dropout1 = nn.Dropout(dropout)
        # feed forward
        self.norm2 = nn.RMSNorm(out_channels)
        self.ffn = nn.Sequential(
            nn.Linear(out_channels, out_channels * 4),
            nn.SiLU(),
            nn.Dropout(dropout),
            nn.Linear(out_channels * 4, out_channels),
        )
        self.dropout2 = nn.Dropout(dropout)
        self.conv2 = nn.Conv1d(out_channels, out_channels, kernel_size, padding=kernel_size // 2)

        # A final convolution for dimension matching the residual (if needed).
        self.residual_conv = (
            nn.Conv1d(inp_channels, out_channels, 1) if inp_channels != out_channels else nn.Identity()
        )
    
    def forward(self, 
                x_feat: Tensor,
                cond_feat: Tensor,) -> Tensor:
        """
        Args:
            x_feat: (B, inp_channels, T)
            cond_feat: (B, L, cond_channels)
        Returns:
            (B, T, out_channels)
        """
        out = self.conv1(self.norm(x_feat))
        # cross attention
        cross_attention_inp = einops.rearrange(out, "b c t -> b t c")
        cross_attention_out = self.cross_attention(self.norm1(cross_attention_inp), cond_feat,)
        cross_attention_out = cross_attention_inp + self.dropout1(cross_attention_out)
        # feed forward
        ffn_inp = cross_attention_out
        ffn_out = self.ffn(self.norm2(ffn_inp))
        ffn_out = ffn_inp + self.dropout2(ffn_out)
        out = einops.rearrange(cross_attention_out, "b t c -> b c t")
        out = self.conv2(out)
        # Residual connection.
        out = out + self.residual_conv(x_feat)

        return out